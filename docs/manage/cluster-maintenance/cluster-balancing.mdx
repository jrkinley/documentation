---
title: Cluster Balancing
---

<head>
    <meta name="title" content="Cluster balancing | Redpanda Docs"/>
    <meta name="description" content="For balanced clusters, Redpanda provide leadership balancing and partition balancing. It also supports manual partition balancing with rpk and with the Kafka API and the Admin API."/>
</head>

When a topic is created, Redpanda evenly distributes its partitions by sequentially allocating them to the node with the least number of partitions. By default, Redpanda provides leadership balancing and partition rebalancing when nodes are added or decommissioned. 

With an Enterprise license, you can additionally enable *Continuous Data Balancing* to continuously monitor node and rack availability and disk usage. This enables self-healing clusters that dynamically balance partitions. It also continuously maintains adherence to rack-aware replica placement policy and self heals after rack (or availability zone) failure or replacement. See [Configure Continuous Data Balancing](../continuous-data-balancing).

Cluster balancing protects you from unbalanced systems that saturate resources on one or more nodes. This can affect throughput and latency. Furthermore, a cluster with replicas on a down node risks availability loss if more nodes fail, and a cluster that keeps losing nodes without healing eventually risks data loss.

## Leadership balancing

Automatic leadership balancing improves cluster performance by transferring leadership for a node's partitions to other replicas. Automatic leadership balancing changes where data is read to and written to first. It doesn't move any data.

Leadership balancing is enabled by default with the `enable_leader_balancer` property.

## Redpanda partition balancing

Partition balancing moves data from hot-loaded nodes to least-loaded nodes to ensure steady-state performance. It ensures predictable and even performance across all nodes in a cluster. Partition balancing is invoked periodically, determined by the `partition_autobalancing_tick_interval_ms` property. Default is 30 seconds.

By default, Redpanda rebalances partition distribution when nodes are added or decommissioned. Continuous Data Balancing additionally rebalances partitions when nodes become unavailable or when disk space usage exceeds a threshold. 

* Monitoring unavailable nodes lets Redpanda self-heal clusters by moving partitions from a failed node to a healthy node.
* Monitoring low disk space lets Redpanda distribute partitions across nodes with enough disk space. If free disk space reaches a critically low level, Redpanda blocks clients from producing. For information about the disk space threshold and alert, see [Handle full disks](../disk-utilization/#handle-full-disks).

### Partition balancing settings

Select your partition balancing setting with the `partition_autobalancing_mode` property. 

| Setting      | Description |
| ----------- | ----------- |
| `node_add`      | Partition balancing happens when nodes are added.  <br /><br />This is the default setting.|
| `continuous`   | In this mode, Redpanda continuously monitors the cluster for node failures and high disk usage. It uses this information to automatically redistribute partitions across the cluster to maintain optimal performance and availability. It also monitors rack availability after failures, and for a given partition, it tries to move excess replicas from racks that have more than one replica to racks where there are none. See [Configure Continuous Data Balancing](../continuous-data-balancing).<br /><br />This option requires an Enterprise license.     |
| `off`     | All partition balancing from Redpanda is turned off. <br /><br />This option is not recommended for production clusters. Only set to `off` if you need to move partitions manually.   |

## Partition balancing with the Kafka API

As an alternative to Redpanda partition balancing, you can change partition assignments explicitly with the Kafka API or with any 3rd-party tool in the Kafka ecosystem that controls partition movement using the Kafka API. 

To reassign partitions with the Kafka API:

1. Set the `partition_autobalancing_mode` property to `off`. If Redpanda partition balancing is enabled, Redpanda may change partition assignments regardless of what you do through the Kafka API.

2. Show initial replica sets. For example, for topic `foo`:
  ```bash
  rpk topic describe foo -p
  PARTITION  LEADER  EPOCH  REPLICAS  LOG-START-OFFSET  HIGH-WATERMARK
  0          1       1      [1 2 3]   0                 645
  1          1       1      [0 1 2]   0                 682
  2          3       1      [0 1 3]   0                 672
  ```

1. Put all partition reassignments in a JSON file. For example, to replace node 0 with another node in the replica sets of partitions 1 and 2:
   
  ```json
  {
    "version": 1,
    "partitions": [
      {
        "topic": "foo",
        "partition": 1,
        "replicas": [
          3,
          1,
          2
        ]
      },
      {
        "topic": "foo",
        "partition": 2,
        "replicas": [
          2,
          1,
          3
        ]
      }
    ]
  }
  ```

4. Execute partition reassignments with the `kafka-reassign-partitions.sh` script. This example uses `example.json` as the name of the JSON file:

  ```bash
  kafka-reassign-partitions.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094,localhost:9095 --reassignment-json-file example.json --execute
  Current partition replica assignment

  {"version":1,"partitions":[{"topic":"foo","partition":1,"replicas":[1,2,0],"log_dirs":["any","any","any"]},{"topic":"foo","partition":2,"replicas":[3,1,0],"log_dirs":["any","any","any"]}]}

  Save this to use as the --reassignment-json-file option during rollback
  Successfully started partition reassignments for foo-1,foo-2
  ```

5. Verify that the reassignment is complete with the flags `--verify --preserve-throttles`: 

  ```bash
  kafka-reassign-partitions.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094,localhost:9095 --reassignment-json-file example.json --verify --preserve-throttles
  Status of partition reassignment:
  Reassignment of partition foo-1 is complete.
  Reassignment of partition foo-2 is complete.
  ```

  Alternatively, run `rpk` again to show your reassigned replica sets:

  ```bash
  rpk topic describe foo -p
  PARTITION  LEADER  EPOCH  REPLICAS  LOG-START-OFFSET  HIGH-WATERMARK
  0          3       2      [1 2 3]   0                 0
  1          2       2      [1 2 3]   0                 0
  2          2       1      [1 2 3]   0                 0
  ```  

### Differences in partition balancing between Redpanda and Kafka

- Kafka comes with a shell script to issue partition reassignments: `kafka-reassign-partitions.sh`. This script attempts to use throttle configurations that Redpanda does not support, such as `replica.alter.log.dirs.io.max.bytes.per.second`. Include the flag `--preserve-throttles` to avoid errors.

- Kafka supports uprelication or dereplication via partition reassignments. Redpanda currently doesn't support this.

- In a partition reassignment, users must provide the node ID for each replica. Kafka validates the node ID for any new replica (that wasn't in the previous replica set) against the list of alive nodes. Redpanda validates all replicas against the list of alive nodes.

- Redpanda supports shard-level partition assignments. Therefore, when resolving a partition reassignment, Redpanda automatically determines the shard placements. If you want a topic-partition on a specific shard, you must use the Admin API.

- When there are two identical partition reassignment requests, Redpanda rejects one of them with the error `no_reassignment_in_progress`.

### Assign partitions with Admin API

You can manually assign partitions at topic creation with the Admin API: 

```bash
kafka-topics.sh --create --bootstrap-server 127.0.0.1:9092 --topic custom-assignment --replica-assignment 0:1:2,0:1:2,0:1:2 
```