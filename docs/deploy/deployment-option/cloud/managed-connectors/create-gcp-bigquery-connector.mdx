---
title: Create a GCP BigQuery Sink Connector
---

<head>
    <meta name="title" content="Create a BigQuery Sink Connector | Redpanda Docs"/>
    <meta name="description" content="Use the Redpanda Cloud UI to create a BigQuery Sink Connector."/>
    <link rel="canonical" href="https://docs.redpanda.com/docs/deploy/deployment-option/cloud/managed-connectors/create-s3-sink-connector/" />
</head>

The GCP BigQuery Sink connector enables you to stream any structured data from
Redpanda to BigQuery for advanced analytics.

## Prerequisites

Before you can create a GCP BigQuery Sink connector in the Redpanda Cloud, you
must:

1. Create a [Google Cloud](https://cloud.google.com/) account.
2. In the **GCP home** page:
   - [Select an existing project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#get_an_existing_project)
     or [create a new one](https://cloud.google.com/resource-manager/docs/creating-managing-projects#creating_a_project).
   - [Create a new dataset](https://cloud.google.com/bigquery/docs/datasets) for the project (enter `user_actions` as the Dataset ID).
   - After creating the dataset, [create a new table](https://cloud.google.com/bigquery/docs/tables) to hold the data you intend to stream from Redpanda Cloud topics.
     Specify a structure for the table using schema values that align with your Redpanda Cloud topic data.

3. Create a [custom role](https://cloud.google.com/iam/docs/creating-custom-roles).

  The role must have the following permissions:

  ```
  bigquery.datasets.get
  bigquery.tables.create
  bigquery.tables.get
  bigquery.tables.getData
  bigquery.tables.list
  bigquery.tables.update
  bigquery.tables.updateData
  ```
4. Create a [service account](https://cloud.google.com/iam/docs/service-accounts-create).
5. [Add the custom role to your service account](https://cloud.google.com/iam/docs/granting-changing-revoking-access).
6. [Create a service account key](https://cloud.google.com/iam/docs/keys-create-delete), and then download it.

## Limitations

The BigQuery Sink connector has the following limitations:

- The connector doesnâ€™t support schemas with recursion.
- When the connector is configured with `Upsert enabled` or `Delete enabled`, it
  does not support Single Message Transformations (SMTs) that modify the topic name.

## Create a GCP BigQuery Sink connector

To create the GCP BigQuery Sink connector:

1. In Redpanda Console, click **Connectors** in the navigation menu, and then
   click **Create Connector**.
2. Select the Google BigQuery managed connector.
3. On the **Create Connector** page, specify the following required connector configuration options:

   | Property                     | Description |
   | ---------------------------- | ----------- |
   | `Topics to export`             | A comma-separated list of the cluster topics you want to replicate to GCP BigQuery. |
   | `Topics regex`                 | A Java regular expression of topics to replicate. For example: specify `.*` to replicate all available topics in the cluster. Applicable only when **Use regular expressions** is selected. |
   | `Credentials JSON`             | A JSON key with BigQuery service account credentials. |
   | `Project`                      | The BigQuery project to which topic data will be written. |
   | `Default dataset`              | The default GCP BigQuery dataset to be used. |

### Advanced GCP BigQuery configuration

In most instances, the preceding basic configuration properties are sufficient.
However, if you require any additional property settings (for example, automatically
create BigQuery tables or map topics to tables), then specify any of the following
_optional_ advanced connector configuration properties by selecting **Show advanced options**
on the **Create Connector** page:

   | Property                     | Description |
   | ---------------------------- | ----------- |
   | `Auto create tables` | Automatically create BigQuery tables if they don't already exist. If the table does not exist, then it is created based on the record schema. |
   | `Topic to table map` | Map of topics to tables. Format: comma-separated tuples, for example `:,:`. |
   | `Allow new BigQuery fields` | If true, new fields can be added to BigQuery tables during subsequent schema updates. |
   | `Allow BigQuery required field relaxation` | If true, fields in the BigQuery schema can be changed from `REQUIRED` to `NULLABLE`. |
   | `Upsert enabled` | Enables upsert functionality on the connector. |
   | `Delete enabled` | Enable delete functionality on the connector.
   | `Time partitioning type` | The time partitioning type to use when creating tables. |

## Test the connection

After the connector is created, go to your BigQuery worksheets and query your
table:

```
SELECT * FROM `project.dataset.table`
```

It may take a couple of minutes for the records to be visible in BigQuery.

## Troubleshoot

GCP credentials are checked for validity during connector creation, upon
clicking **Finish**. In cases where there are invalid credentials, the connector
is not created.

Other issues are reported using a failed task error message.

| Message | Action |
|---------|--------|
| **Not found: Project invalid-project-name** | Check to make sure `Project` contains a valid BigQuery project. |
| **Not found: Dataset project:invalid-dataset** | Check to make sure `Default dataset` contains a valid BigQuery dataset. |
| **An unexpected error occurred while validating credentials for BigQuery: Failed to create credentials from input stream** | The credentials given as a JSON file in the `Credentials JSON` property are incorrect. Copy a valid key from the Google Cloud service account. |
| **JsonConverter with schemas.enable requires "schema" and "payload" fields** | The connector encountered an incorrect message format when reading from a topic. The JSON messages should contain schema in the `schema` field and payload in the `payload` field. |
| **JsonParseException: Unrecognized token 'test': was expecting JSON** | During reading from a topic the connector encountered a message that is invalid JSON. The JSON messages should contain schema in the `schema` field and payload in the `payload` field. |

## Suggested reading

[External BigQuery sink connector](https://github.com/wepay/kafka-connect-bigquery/wiki/Connector-Configuration) documentation
