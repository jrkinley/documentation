---
title: Automate Deployment for Production
---

<head>
    <meta name="title" content="Automate Deployment for Production | Redpanda Docs"/>
    <meta name="description" content="Automate Deployment for Production."/>
    <link rel="canonical" href="https://docs.redpanda.com/docs/deploy/deployment-option/self-hosted/manual/production/production-deployment-automation/" />
</head>

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

If you use automation tools like Terraform and Ansible in your environment, you can use them to quickly provision a Redpanda cluster for production. Terraform can set up the infrastructure and output a properly-formatted `hosts.ini` file, and Ansible can use that `hosts.ini` file as input to install Redpanda. Or, you can supply your own hosts file (without using Terraform), but use Ansible to install Redpanda.

## Use Terraform to set up infrastructure

If you haven't yet installed Terraform, do so following the [Terraform documentation](https://learn.hashicorp.com/tutorials/terraform/install-cli).

1. Clone the [`deployment-automation` GitHub repository](https://github.com/redpanda-data/deployment-automation/):

 ```bash
 git clone https://github.com/redpanda-data/deployment-automation.git
 ```

3. Change into the directory:

 ```bash
 cd deployment-automation
 ```

1. Follow the instructions for your preferred cloud provider. The recommended [Terraform registry for Redpanda](https://registry.terraform.io/modules/redpanda-data/redpanda-cluster/aws/latest) deploys VMs on AWS EC2. To create an AWS Redpanda cluster, review the [default variables](https://github.com/redpanda-data/deployment-automation/blob/main/aws/main.tf) and make any edits necessary for your environment. 


<Tabs>
  <TabItem value="aws" label="AWS" default>

  1. Within the `deployment-automation` folder, change into the `aws` directory:

    ```bash
    cd aws
    ```

  2. Set AWS credentials. Terraform provides multiple ways to set the AWS secret and key. See the [Terraform documentation](https://registry.terraform.io/providers/hashicorp/aws/latest/docs#environment-variables).

  3. Initialize Terraform:

    ```bash
    terraform init
    ```


  4. Create the cluster with `terraform apply`. 
     - If you don't have a default VPC defined, then you need to set a VPC ID and subnet ID.
     - If the public key path isn't the default `~/.ssh/id_rsa.pub`, then you need to set it. 
    
      ```bash
      terraform apply -var='public_key_path=~/.ssh/id_rsa.pub' -var='subnet_id=<subnet-ID>' -var='vpc_id=<VPC-ID>'
      ```

    The following example creates a three-broker cluster using i3.large instances:

    ```bash
    terraform apply -var="instance_type=i3.large"
    ```
    
    Configuration options:

    | Property      | Description |
    | ----------- | ----------- |
    | `aws_region` | The AWS region to use for deploying the infrastructure. Default: `us-west-2` |
    | `nodes` | The number of brokers to base the cluster on. Default: `3` |
    | `enable_monitoring` | Creates a Prometheus/Grafana instance for monitoring the cluster. Default: `true` |
    | `instance_type` | The instance type on which Redpanda is deployed. Default: `i3.8xlarge` |
    | `prometheus_instance_type` | The instance type on which Prometheus and Grafana are deployed. Default: `c5.2xlarge` |
    | `public_key_path` | Path to the public key of the keypair used to access the brokers. Default: `~/.ssh/id_rsa.pub` |
    | `distro` | Linux distribution to install. (This affects the `distro_` variables.) Default: `ubuntu-focal` |
    | `distro_ami` | AWS AMI to use for each available distribution. These must be changed according to the chosen AWS region. |
    | `distro_ssh_user` | User used to ssh into the created EC2 instances. |


  </TabItem>

  <TabItem value="gcp" label="GCP">

  1. Within the `deployment-automation` folder, change into the `gcp` directory:

    ```bash
    cd gcp
    ```

  2. You need an existing subnet to deploy the VMs into. The subnet's attached firewall should allow inbound traffic on ports 22, 3000, 8082, 8888, 8889, 9090,  9092, 9644, and 33145. This module adds the `rp-node` tag to the deployed VMs, which can be used as the target tag for the firewall rule.

  3. Initialize Terraform:

    ```bash
    terraform init
    ```

  4. Create the cluster: 
 
    `terraform apply`

    The following example creates a three-broker cluster using the subnet named `redpanda-cluster-subnet`:

    ```bash
    terraform apply -var nodes=3 -var subnet=redpanda-cluster-subnet -var public_key_path=~/.ssh/id_rsa.pub -var ssh_user=$USER
    ```
   
    Configuration options:
   
    | Property      | Description |
    | ----------- | ----------- |
    | `region` | The region to use for deploying the infrastructure. Default: `us-west1` |
    | `zone` | The region's zone to deploy the infrastructure. Default: `a` |
    | `subnet` | The name of an existing subnet to deploy the infrastructure. |
    | `nodes` | The number of brokers to base the cluster on. Keep in mind that one broker is used as a monitoring broker. Default: `1`|
    | `disks` | The number of local disks to deploy on each machine. Default = `1`|
    | `image` | The OS image running on the VMs. Default: `ubuntu-os-cloud/ubuntu-1804-lts` |
    | `machine_type` | The machine type. Default: `n2-standard-2`|
    | `public_key_path` | Path to the public key of the keypair used to access the brokers. |
    | `ssh_user` | The ssh user. Must match the one in the public ssh key's comments. |


  </TabItem>

</Tabs>

5. Now you can install Redpanda either with Ansible or with the [Redpanda installation binary](../../production/production-deployment).

:::note
Terraform code is also included for [Azure](https://github.com/redpanda-data/deployment-automation/blob/main/azure/README.md) and [IBM Cloud](https://github.com/redpanda-data/deployment-automation/blob/main/ibm/README.md):
```bash
export AWS_ACCESS_KEY_ID=<your-key-ID>
export AWS_SECRET_ACCESS_KEY=<your-secret-access-key>

# Go to the cloud provider directory (aws, gcp, azure, or IBM)
cd azure
terraform init
terraform apply --auto-approve
```
:::


## Use Ansible to install Redpanda

If you haven't yet installed Ansible, do so following the [Ansible documentation](https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html). Depending on your system, you may also need to install Python packages, like SELinux or JMESPath.

1. Set required Ansible variables:

  ```bash
  export CLOUD_PROVIDER=<cloud-provider>
  export ANSIBLE_COLLECTIONS_PATHS=${PWD}/artifacts/collections
  export ANSIBLE_ROLES_PATH=${PWD}/artifacts/roles
  export ANSIBLE_INVENTORY=${PWD}/${CLOUD_PROVIDER}/hosts.ini
  ```

1. Clone the [`deployment-automation` GitHub repository](https://github.com/redpanda-data/deployment-automation/):

   ```bash
   git clone https://github.com/redpanda-data/deployment-automation.git
   ```

1. Change into the directory:

  ```bash
  cd deployment-automation
  ```

1. Install the roles required by Ansible:

  ```bash
  ansible-galaxy install -r ansible/requirements.yml
  ```

### Configure a hosts file

If you used Terraform to deploy the instances, the `hosts.ini` is configured automatically in the [`artifacts`](https://github.com/redpanda-data/deployment-automation/tree/main/artifacts) directory. 

If you didn't use Terraform, then you must manually update the [redpanda] section. When you open the file, you see something like the following:

```ini
[redpanda]
ip ansible_user=ssh_user ansible_become=True private_ip=pip id=0
ip ansible_user=ssh_user ansible_become=True private_ip=pip id=1

[monitor]
ip ansible_user=ssh_user ansible_become=True private_ip=pip id=1
```

Under the `[redpanda]` section, replace the following:

| Property      | Description |
| ----------- | ----------- |
| `ip` | The public IP address of the machine. |
| `ansible_user` | The username for Ansible to use to ssh to the machine. |
| `private_ip` | The private IP address of the machine. This could be the same as the public IP address. |
| `id` | The node ID of the Redpanda instance. This must be unique for each host. |

You can add additional properties to configure features like rack awareness and Tiered Storage. The `[monitor]` section is only relevant if you have Prometheus and Grafana installed on a given host. If you don't want to have this deployed, then remove the `[monitor]` section.

### Run a playbook

Use the [Ansible collection for Redpanda](https://galaxy.ansible.com/redpanda/cluster) to build a Redpanda cluster. The recommended Redpanda playbook enables TLS encryption and Tiered Storage. 

If you prefer, you can download the modules and required roles and create your own playbook. For example, if you want to handle your own data directory, you can toggle that part off, and Redpanda just makes sure the permissions are correct. If you want to generate your own security certificates, you can. 

To run the recommended Redpanda playbook and have a fully-running Redpanda cluster in one command:

 ```bash
 ansible-playbook --private-key <your-private-key> -i hosts.ini -v ansible/playbooks/provision-node.yml
 ```

To use your own playbook, replace `provision-node.yml` with your playbook name. 

:::note
If you use a playbook to create a cluster, also use it for any subsequent operations, like upgrades.
:::

#### Additional Redpanda values

You can pass the following variables as `-e var=value` when running Ansible:

| Property      | Default value | Description |
| ----------- | ----------- | ----------- |
| `redpanda_organization` | `redpanda-test` | Set this to identify your organization in the asset management system. |
| `redpanda_cluster_id` | `redpanda` | Helps identify the cluster. |
| `advertise_public_ips` | `false` | Configure Redpanda to advertise the node's public IPs for client communication instead of private IPs. This enables using the cluster from outside its subnet. Note: This is not recommended for production deployments, because it means that your nodes will be public. 
| `grafana_admin_pass` | <your_secure_password> | Configure Grafana's admin user's password. |
| `ephemeral_disk` | `false` | Enable filesystem check for attached disk. This is useful when using attached disks in instances with ephemeral OS disks like Azure L Series. This allows a filesystem repair at boot time and ensures that the drive is remounted automatically after a reboot. |
| `redpanda_mode` | `production` | Enables hardware optimization. |
| `redpanda_admin_api_port` | `9644` |  |
| `redpanda_kafka_port` | `9092` |  |
| `redpanda_rpc_port` | `33145` |  |
| `redpanda_use_staging_repo` | `false` | Enables access to unstable builds. |
| `redpanda_version` | `latest` | For example, 22.2.2-1 or 22.3.1~rc1-1. If this value is set, then the package is upgraded if the installed version is lower than what has been specified. |
| `redpanda_rpk_opts` |  | Command line options to be passed to instances where `rpk` is used on the playbook; for example, superuser credentials can be specified as "--user myuser --password mypassword". |
| `redpanda_install_status` | `present` | If `redpanda_version` is set to latest, changing `redpanda_install_status` to latest causes an upgrade; otherwise, the currently-installed version remains. |
| `redpanda_data_directory` | `/var/lib/redpanda/data` | Path where Redpanda keeps its data. |
| `redpanda_key_file` | `/etc/redpanda/certs/node.key` | TLS: Path to private key. |
| `redpanda_cert_file` | `/etc/redpanda/certs/node.crt` | TLS: Path to signed certificate. |
| `redpanda_truststore_file` | `/etc/redpanda/certs/truststore.pem` | TLS: Path to truststore. |
| `tls` | `false` | Set to true to configure Redpanda to use TLS. This can be set on a per-node basis, although this may lead to errors configuring `rpk`. |
| `skip_node` |	`false`	| Per-node config to prevent the Redpanda_broker role being applied to this specific node. Use carefully when adding new nodes to avoid existing nodes from being reconfigured. |
| `restart_node` | `false` | Per-node config to prevent Redpanda brokers from being restarted after updating. Use with care: this can cause `rpk` to be reconfigured but the node not be restarted and therefore be in an inconsistent state. |
| `rack` | `undefined` | Per-node config to enable rack awareness. Rack awareness is enabled cluster-wide if at least one node has the rack variable set. |
| `tiered_storage_bucket_name` |  | Set bucket name to enable Tiered Storage. |
| `aws_region` |  | The region to be used if Tiered Storage is enabled. |

#### Custom configuration

You can specify any available Redpanda configuration value (or set of values) by passing a JSON dictionary as an Ansible `extra-var`. These values are spliced with the calculated configuration and only override the values that you specify. Values must be unset manually with `rpk`. There are two sub-dictionaries you can specify: `redpanda.cluster` and `redpanda.node`. For more information, see [Cluster Configuration Properties](../../../../../../reference/cluster-properties) and [Node Configuration Properties](../../../../../../reference/node-properties). 

```bash
export JSONDATA='{"cluster":{"auto_create_topics_enabled":"true"},"node":{"developer_mode":"false"}}'
ansible-playbook ansible/<playbook-name>.yml --private-key artifacts/testkey -e redpanda="${JSONDATA}"
```

:::note
Adding whitespace breaks configuration merging. 
:::

Use `rpk` and standard Kafka tools to produce and consume from the Redpanda cluster.

#### Configure Grafana

To deploy a Grafana node, you must have a `[monitor]` section in your hosts file. The Grafana URL is `http://<grafana-host>:3000/login`.

To run the `deploy-prometheus-grafana.yml` playbook:

```bash
ansible-playbook ansible/deploy-prometheus-grafana.yml \
-i hosts.ini \
--private-key '<path-to-a-private-key-with-ssh-access-to-the-hosts>'
```

#### Build the cluster with TLS enabled

Configure TLS with externally-provided and signed certificates (possibly through a corporate-provided cert-manager). Then run the `provision-tls-cluster` playbook, specifying the certificate locations on new hosts. You can either pass the variables in the command line or edit the file and pass them there. Consider whether you want public access to the Kafka API and Admin API endpoints. For example:

```bash
ansible-playbook ansible/provision-tls-cluster.yml \
-i hosts.ini \
--private-key '<path-to-a-private-key-with-ssh-access-to-the-hosts>' \
--extra-vars create_demo_certs=false \
--extra-vars advertise_public_ips=false \ 
--extra-vars handle_certs=false \
--extra-vars redpanda_truststore_file='<path-to-ca.crt-file>' 
```

#### Add nodes to an existing cluster

To add nodes to a cluster, you must add them to the hosts file and run the relevant playbook again. You can add `skip_node=true` to the existing hosts to avoid the playbooks being rerun on them.

#### Upgrade a cluster

The playbook is designed to be idempotent, so it should be suitable for running as part of a CI/CD pipeline or through Ansible Tower. Upgrade support is built in, and the playbook is capable of upgrading the packages and then performing a rolling upgrade across the cluster.

:::note notes
- Because any changes made to properties outside of the playbook may be overwritten by this procedure, these settings should be incorporated as part of the provided `--extra-vars`. For example, `--extra-vars=enable_tls=true`. See [Custom configuration](#custom-configuration).
- Test that your upgrade path is safe before using it in production.
:::

To upgrade a cluster, run the playbook with a specific target version. If the target version is higher than the currently installed version, then the cluster is upgraded and restarted automatically:

```bash
ansible-playbook --private-key ~/.ssh/id_rsa ansible/<playbook-name>.yml -i hosts.ini -e redpanda_version=22.3.10-1 
```

By default, the playbook selects the latest version of the Redpanda packages, but an upgrade is only performed if the `redpanda_install_status` variable is set to `latest`:

```bash
ansible-playbook --private-key ~/.ssh/id_rsa ansible/<playbook-name>.yml -i hosts.ini -e redpanda_install_status=latest 
```

It's possible to upgrade clusters where SASL authentication has been turned on. For this, you need to additionally specify the `redpanda_rpk_opts` variable to include to username and password or a superuser or appropriately privileged user. For example:

```bash
ansible-playbook --private-key ~/.ssh/id_rsa ansible/<playbook-name>.yml -i hosts.ini --extra-vars=redpanda_install_status=latest --extra-vars "{
\"redpanda_rpk_opts\": \"--user ${MY_USER} --password ${MY_USER_PASSWORD}\"
}"
```

Similarly, you can put the `redpanda_rpk_opts` into a YAML file protected with Ansible vault.

```bash
ansible-playbook --private-key ~/.ssh/id_rsa ansible/<playbook-name>.yml -i hosts.ini --extra-vars=redpanda_install_status=latest --extra-vars @vault-file.yml --ask-vault-pass
```

##### Migrate to the AWS module

Redpanda previously delivered AWS code as simple in-place Terraform code, but it is now converted to a module. To migrate from the old AWS code:

1. Download the new configuration.
2. Run the following:
   
```bash
terraform state list | while read -r line ; do terraform state mv "$line" "module.redpanda-cluster.$line"; done
```

### Troubleshooting

On Mac OS X, Python may be [unable to fork workers](https://stackoverflow.com/questions/50168647/multiprocessing-causes-python-to-crash-and-gives-an-error-may-have-been-in-progr). If you see something like this:

```bash
ok: [34.209.26.177] => {“changed”: false, “stat”: {“exists”: false}}
objc[57889]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called.
objc[57889]: +[__NSCFConstantString initialize] may have been in progress in another thread when fork() was called. We cannot safely call it or ignore it in the fork() child process. Crashing instead. Set a breakpoint on objc_initializeAfterForkError to debug.
ERROR! A worker was found in a dead state
```

To resolve this, try setting an environment variable: 

```bash
export OBJC_DISABLE_INITIALIZE_FORK_SAFETY=YES
```