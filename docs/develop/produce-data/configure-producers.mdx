---
title: Configure Producers
---

<head>
  <meta name="title" content="Configure Producers | Redpanda Docs" />
  <meta
    name="description"
    content="Learn concepts about Kafka producers and how to configure them when developing with Redpanda."
  />
</head>

import Tabs from "@theme/Tabs";
import TabItem from "@theme/TabItem";

This section introduces concepts about Kafka producers and gives guidelines for configuring them when developing with Redpanda:

- Customizing reliable message delivery with acknowledgements
- Increasing throughput with message batching and compression
- Enabling exactly-once message delivery with idempotent producers
- Writing a set of messages atomically with transactions

## Producer concepts

A Kafka _producer_ is a client application that publishes events, or messages, for one or more Kafka topics. A Redpanda cluster's brokers receive and store events written by producers. Producers communicate with Redpanda through the Kafka Producer API. Published messages of a topic are consumed by Kafka consumers that subscribe to it.

A producer can publish messages for one or more topics. Given that a topic can be partitioned across one or more brokers (with each partition of a topic assigned to a different broker), a producer can publish messages to different partitions in parallel. Having more partitions for a topic may increase throughput.

### Partitions and replicas

A producer controls which partitions it publishes to. Messages of a single topic are sent to different partitions, based on a partition index specified when publishing a message, or a partitioning strategy of the producer's _partitioner_. Kafka's built-in partitioner assigns a message to a partition based on the message's key:

- If a message's key is null, a partitioner assigns partitions in a round-robin fashion (Kafka 2.3 and earlier) or in a sticky fashion (Kafka 2.4 and later).
  - A round-robin partitioner optimizes for the equal distribution of messages amongst partitions.
  - A sticky partitioner optimizes for larger message batches, where messages of the same batch are sent to the same "sticky" partition.
- Otherwise, if a message's key is non-null, the partitioner assigns partitions based on the key's hash. Messages with the same key (hash) are assigned to the the same partition. By default, the partitioner hashes keys with the murmur2 algorithm.

A partition stores messages in the order they were published, and a consumer reads messages from a partition also in the order they were published. Ordering of messages across producers however is arbitrary.

Because a partition is usually replicated across different brokers, a producer sends messages to only one broker of the partition, the _leader_. The leader receives messages from the producer, and the other replicas, the _followers_, receive copies of the messages from the leader. A replica that has successfully received all published messages is an _in-sync replica_. A leader is always an in-sync replica, and a follower that hasn't received all published messages is an out-of-sync replica. A leader knows whether a follower is in-sync or out-of-sync.

### Send request

A producer call to `send()` produces a message that is:
- Processed by interceptors (if configured)
- Serialized
- Assigned to a partition
- Compressed (if configured)
- Added to a batch (if configured)

The `send()` call may block when being intercepted, serialized, partition-assigned, or compressed. If being batched, a `send()` call won't send a message over the network until the batch is full, the time to wait to coalesce messages has elapsed, another message to the same broker is about to be sent and it has space for this message, or the producer is being flushed.

A broker can respond to a send with a success or an error. The returned error can be either resolvable or unresolvable with a retry. If resolvable with a retry, and the producer is configured to retry, the producer can resend the message.

### Reliable delivery

The level of reliable delivery of a producer is determined by whether a published message is guaranteed to be received by a Redpanda cluster before the producer discards it.

- A best-effort level of delivery doesn't guarantee successful reception. A producer writes a message once and can immediately discard it without waiting for an acknowledgement of its reception. A leader must receive the message on its initial send, or the message is lost. Use cases that send periodic updates of the latest value can use best-effort, no acknowledgement reliability. Without the overhead of acknowledgements (or message retries), this use case trades-off the possibility of data loss for higher throughput and lower latency.

- A reliable level of delivery guarantees at least one broker has successfully received a message. A producer writes a message and expects either the leader or a quorum of insync-replicas to acknowledge message reception. When a message isn't acknowledged, a producer retries sending it. Use cases that require higher levels of message availability and fault tolerance can use reliable delivery with one or more acknowledgements of message reception. So while waiting for messages to be acknowledged increases latency and decreases throughput, the risk of data loss is minimized.

### Exactly-once delivery

A producer may send a message more than once, and that may result in a broker (or a consumer) receiving the message more than once. For example, a broker may fail to acknowledge a message after it has received it, and a producer may retry multiple times and send duplicate messages.

In Kafka, a producer can be configured or programmed to enable _idempotence_ so that messages are received exactly once by brokers and consumers.

Moreover, some applications produce a set of messages that are a single logical batch that must be received and processed exactly once. Any error in processing any message in the set invalidates all messages in the set.

In Kafka, a producer application can create a _transaction_ with a set of messages that are written atomically.

#### Idempotence

A producer with _idempotence_ enabled can send a message multiple times and have it received by a Redpanda broker just once.

When a producer sends messages to a topic, each message should be recorded only once in the order in which it was sent. However, network issues such as a connection failure can result in a timeout, which prevents a send request from succeeding. In such cases, the client retries the send request until one of these events occurs:

* The client receives an acknowledgment from the broker that the send was successful.
* The retry limit is reached.
* The message delivery timeout limit is reached.

Since there is no way to tell if the initial send request succeeded before the disruption, a retry can result in a duplicate message. A retry can also cause subsequent messages to be received out of order.

An idempotent producer prevents this problem by assigning a unique ID to every send request. The request ID consists of the producer ID and a sequence number. The sequence number identifies the order in which each write request was sent. If a retry results in a duplicate message, a Redpanda broker detects and rejects the duplicate message and maintains the original order of the messages.

If new send requests continue while a previous request is being retried, the new requests are stored in the client’s memory in the order in which they were sent. The client must also retry these requests once the previous request is successful.

#### Transactions

A producer application can call Kafka producer APIs to create an atomic set of writes to multiple topics and partitions. All send() calls of a transaction must succeed in order for the transaction to be committed. If any send() fails, the transaction's commit fails, and all messages are dropped.

On a broker, a _transaction coordinator_ communicates with a producer and tracks when a transaction begins, commits, or aborts. It writes the state of a transaction in a transaction log, and when the producer commits or aborts a transaction, it executes the commit protocol.

### Message batching and compression

Sending small messages over the network can underutilize the available network bandwidth. To improve network efficiency and throughput, a producer can coalesce multiple messages into a single batch to send over the network. To optimize the balance of throughput and latency, both the maximum size of a batch and the maximum time to wait for messages to coalesce into a batch can be configured.

A producer can also apply data compression to its messages to further improve bandwidth efficiency. Messages with repeated data structures, and batches containing similar messages, have better compression ratios.

## Producer configuration

This section describes how to use some of the common producer configuration properties.

For a reference list of producer configuration properties, see [Kafka producer configuration properties].

### Configure reliable delivery

The number of acknowledgements for each published message that a producer expects a leader to receive before completing the send request is configured by the `acks` property.

The valid values of `acks`:

- [`acks=0`](#acks0): don't wait for any acknowledgement.
- [`acks=1`](#acks1): wait for acknowledgement from the leader itself.
- [`acks=all`](#acksall): wait for acknowledgment from in-sync replicas.

If acknowledgements are expected but not received, a producer can be configured to resend a message up to a maximum limit with the [`retries`](#retries) property, and it can be configured to have a maximum number of in-flight resent messages with the [`max.in.flight.requests.per.connection`](#maxinflightrequestsperconnection) property.

#### acks=0

The producer doesn’t wait for acknowledgments from the leader and doesn’t retry
sending messages. This increases throughput and lowers latency of the system at
the expense of durability and data loss.

This option allows a producer to immediately consider a message acknowledged when
it is sent to the Redpanda broker. This means that a producer does not have to wait
for any kind of response from the Redpanda broker. This is the least safe option
because a leader-node crash can cause data loss if the data has not yet
replicated to the other nodes in the replica set. However, this setting is useful
in cases where you want to optimize for the highest throughput, and are willing
to risk some data loss.

Because of the lack of guarantees, this setting is the most network bandwidth-
efficient. This makes it useful for use cases like IoT/sensor data collection,
where updates are periodic or stateless and you can afford some degree of data
loss, but prefer gathering as much data as possible in a given time interval.

#### acks=1

The producer waits for an acknowledgment from the leader, but it doesn’t wait
for the leader to get acknowledgments from followers. This setting doesn’t
prioritize throughput, latency, or durability. Instead, `acks=1` attempts to
provide a balance between all of them.

Replication is not guaranteed with this setting because it happens in the
background, after the acknowledgement is sent to the producer from the leader
broker. This setting could result in data loss if the leader broker crashes
before any followers manage to replicate the message.

#### acks=all

The producer waits for the leader to receive acknowledgements from in-sync replicas. This increases durability at the expense of lower throughput and increased latency.

Sometimes referred to as `acks = -1`, `acks=all` configures a leader to consider
message reception as complete when the message has been replicated to the minimum number of in-sync replicas (`min.insync.replicas`) configured for the broker or topic. For Kafka, replication involves calling `fsync`, so as soon as `fsync` is complete, the message is considered acknowledged and
is made visible to readers.

:::info
This parameter has an important distinction compared to Kafka's behavior. In
Kafka, a message is considered acknowledged without the requirement that it has
been fsynced. Messages that have not been flushed to disk will be lost in the
event of a broker crash. So when using `acks=all`, the Redpanda default
configuration is more resilient than Kafka's.
:::

#### retries

This parameter controls the number of times a message is resent to the broker
if the broker fails to acknowledge it. This is essentially the same
as if the client application resends the erroneous message after receiving an
error response. The default value of `retries` in most client libraries is 0.
This means that if the send fails, the message is not re-sent at all.

If you increase this number to a higher value, be sure to take a look at the
`max.in.flight.requests.per.connection` value as well, as leaving that parameter
at its default value can potentially cause ordering issues in the target topic
where the messages arrive. This occurs if two batches are sent to a single
partition and the first fails and is retired, but the second succeeds so the
records in the second batch may appear first.

#### max.in.flight.requests.per.connection

This parameter controls how many unacknowledged messages are allowed
to be sent to the broker simultaneously at any given time. The default value is 5. If you set this parameter to 1, then the producer will not send any more
messages until the previous one is either acknowledged or an error happens, which
can prompt a retry. If you set this parameter to a value higher than 1, then the
producer will send more messages at the same time, which can help to increase
throughput, but add a risk of message reordering if retries are enabled.

In cases when you configure the producer to be [idempotent](../../produce-data/idempotent-producers).
up to five requests can be guaranteed to be in flight with the order preserved.

### Configure idempotent producers

To make producers idempotent, the `enable.idempotence` property must be set to `true` in your producer configuration, as well as in the Redpanda cluster configuration, where it is set to `true` by default.

Some Kafka clients have `enable.idempotence` set to `false` by default. In this case, set the property to `true` by following the instructions for your particular client.

Idempotence is guaranteed within a session. A session starts once a producer is created and a connection is established between the client and the Kafka broker.

:::note

Idempotent producers retry unsuccessful send requests automatically. If you manually retry a send request, the client will assign a new ID to that request, which may lead to duplicate messages.

:::

To disable idempotence (and risk duplicate messages as a result of retries), set `enable_idempotence` to `false`. For general instructions on how to edit any cluster property, refer to [Configuring cluster properties](../../../manage/cluster-maintenance/cluster-property-configuration).

To guarantee true idempotent behavior, you must also set `acks=all` to ensure all replicas record messages in order, even in the event of node failures. In this configuration, both the producer and the broker prefer safety and durability over throughput.

### Configure message batching

Batching is an efficient way to save on both network bandwidth and disk size as
messages can be compressed easier.

When a producer prepares to send messages to a broker, it first fills up a
buffer. When this buffer is full, the producer compresses (if instructed to do
so) and sends out this batch of messages to the broker. The number of batches
that can be sent in a single request to the broker is limited by the
`max.request.size` parameter. The number of requests that can simultaneously be
in this "sending" state is controlled by the
`max.in.flight.requests.per.connection` value, which defaults to 5 in most
client libraries.

Tune the batching configuration using:

#### buffer.memory

The `buffer.memory` property sets the maximum amount of memory available
to the producer for buffering. In a case where messages are sent faster than
they can be delivered to the broker, the producer application may run out of
memory, which will cause it to either block subsequent send calls or even throw
an exception. The `max.block.ms` parameter controls the amount of time the
producer will block before throwing an exception if it cannot immediately send
messages to the broker.

#### batch.size

The `batch.size` property sets the maximum size in bytes of a batched message in one request. The producer will automatically put messages being sent to the same partition into one batch. This configuration parameter is given in bytes as opposed to the number of messages.

When the producer is gathering messages to assign to a batch, at some point it
will hit this byte-size limit, which triggers it to send the batch to the broker.
However, the producer does not necessarily wait (for as much time as set using
`linger.ms`) until the batch is full. Sometimes, it can even send single-message
batches. This means that setting the batch size too large is not necessarily
undesirable, as it won't cause throttling when sending messages; rather, it will
only cause increased memory usage.

Conversely, setting the batch size too small can cause the producer to send
batches of messages faster, which can cause network overhead, meaning a reduced
throughput. The default value is usually 16384, but you can set this as low as 0,
which turns off batching entirely.

#### `linger.ms`

`linger.ms` controls the maximum amount of time the producer will wait before
sending out a batch of messages, if it is not already full. This means you can
somewhat force the producer to make sure that batches are being filled up as
efficiently as possible.

If you are willing to tolerate some latency, setting this value to a number
larger than the default of `0` will cause the producer to send fewer, more
efficient batches of messages. If you set the value to `0`, there is still a
high chance messages arrive around the same time to be batched together.

### Configure serializers

Serializers are responsible for converting a message to a byte array. You can
influence the speed/memory efficiency of your streaming setup by choosing one of
the built-in serializers or writing a custom one. The performance consequences
of using serializers is not typically significant.

For example, if you opt for the JSON serializer, you will have more data to
transport with each message because every record will contain its schema in a
verbose format, which impacts your compression speeds and network throughput.
Alternatively, going with AVRO or Protobuf allows you to only define the schema
in one place, while also enabling features like schema evolution.

## Producer optimization strategies

You can optimize for speed (throughput and latency) or safety (durability and
availability) by adjusting parameters. Finding the optimal configuration depends
on your use case and requires some trial and error. The strategies described here
are meant to be used as guidelines, not hard rules.

There are a wide variety of configuration options within Redpanda. The
configuration options mentioned here work best when combined with other
broker and consumer configuration options. For details, refer to
[Configuring Node Properties](../../../deploy/deployment-option/self-hosted/manual/node-property-configuration)
and [Consumer Offsets](../../consume-data/consumer-offsets).

### Optimizing for speed

From a producer perspective, when you want to get data into
Redpanda as quickly as possible, you can maximize throughput in a variety of ways.
You can set other components’ parameters, like experimenting with the topic
partition size. You can also test [acks](#producer-acknowledgement-settings) settings.

For example, the quicker a producer receives a reply from the broker that the
message has been committed, the sooner it can send the next message, which
generally results in higher throughput. Hence, if you set `acks=1`, then the
leader broker would not have to wait for replication to occur, and it can reply
as soon as it is finished committing the message. As mentioned earlier, this
can result in less durability overall.

Another option to explore is how the producer batches messages. Increasing the
value of `batch.size` and `linger.ms` can increase throughput by making the
producer add more messages into one batch before sending it to the broker and
waiting until the batches can properly fill up. This approach negatively impacts
latency though. By contrast, if you minimize `linger.ms` (for example, to `0`)
and `batch.size` to `1`, you can achieve lower latency, but sacrifice throughput.

### Optimizing for safety

For applications where you must guarantee that there are no lost messages,
duplicates, or service downtime, you can use higher durability `acks` settings.
If you set `acks=all`, then the producer will wait for a majority of replicas to
acknowledge the message before it can send the next message, resulting in lower
latency, because there is more communication required between brokers. This
approach can guarantee higher durability because the message will be replicated
to all brokers.

You can also increase durability by increasing the number of retries the broker
is allowed to make in case messages are not delivered successfully. The trade-off
is that you may allow duplicates to enter the system and potentially alter the
ordering of messages.
